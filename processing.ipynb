{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function # Imports from __future__ since we're running Python 2\n",
    "import os\n",
    "import numpy as np\n",
    "import collections as cls\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import itertools\n",
    "import math\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from matplotlib import pylab\n",
    "from pylab import *\n",
    "random_state = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amsterdam', 'berlin', 'copenhagen', 'dublin', 'edinburgh', 'helsinki', 'lisbon', 'london', 'madrid', 'moscow', 'rome', 'tallin', 'vienna', 'paris', 'reykjavik']\n"
     ]
    }
   ],
   "source": [
    "#get all city names\n",
    "all_cities_paths =  glob.glob(os.path.join(os.getcwd(), 'data','*'))\n",
    "cities = []\n",
    "for city in all_cities_paths:\n",
    "    cities.append(city.split(\"/\")[-1])\n",
    "print(cities)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#finds the not common elements of the two lists\n",
    "#input: list1,list2 --> the lists to be compared\n",
    "#output: the not common elements\n",
    "def find_not_common_columns(list1,list2):\n",
    "    not_common_cols = []\n",
    "    both_lists = list1 + list2\n",
    "    for col in set(both_lists):\n",
    "        if col not in set(list1) or col not in set(list2):\n",
    "            not_common_cols.append(col)\n",
    "    return not_common_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "amsterdam\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location', 'name']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location', 'name']\n",
      "\n",
      "amsterdam's dataset length: 184551\n",
      "\n",
      "\n",
      "\n",
      "berlin\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location', 'name']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location', 'name']\n",
      "\n",
      "berlin's dataset length: 250372\n",
      "\n",
      "\n",
      "\n",
      "copenhagen\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "\n",
      "copenhagen's dataset length: 156019\n",
      "\n",
      "\n",
      "\n",
      "dublin\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "\n",
      "dublin's dataset length: 62835\n",
      "\n",
      "\n",
      "\n",
      "edinburgh\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location', 'name']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location', 'name']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location', 'name']\n",
      "\n",
      "edinburgh's dataset length: 73757\n",
      "\n",
      "\n",
      "\n",
      "helsinki\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['property_type', 'survey_id', 'bathrooms', 'city', 'country', 'location', 'name']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['property_type', 'survey_id', 'bathrooms', 'city', 'country', 'location', 'name']\n",
      "\n",
      "helsinki's dataset length: 16364\n",
      "\n",
      "\n",
      "\n",
      "lisbon\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location', 'name']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location', 'name']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location', 'name']\n",
      "\n",
      "lisbon's dataset length: 120632\n",
      "\n",
      "\n",
      "\n",
      "london\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "\n",
      "london's dataset length: 521224\n",
      "\n",
      "\n",
      "\n",
      "madrid\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "\n",
      "madrid's dataset length: 22798\n",
      "\n",
      "\n",
      "\n",
      "moscow\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "\n",
      "moscow's dataset length: 35914\n",
      "\n",
      "\n",
      "\n",
      "rome\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "\n",
      "rome's dataset length: 289675\n",
      "\n",
      "\n",
      "\n",
      "tallin\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "\n",
      "tallin's dataset length: 10815\n",
      "\n",
      "\n",
      "\n",
      "vienna\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "\n",
      "vienna's dataset length: 42146\n",
      "\n",
      "\n",
      "\n",
      "paris\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "\n",
      "paris's dataset length: 701945\n",
      "\n",
      "\n",
      "\n",
      "reykjavik\n",
      "\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "no difference in the columns\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "DIFFERENCE IN THE COLUMNS REPORTED\n",
      "NOT COMMON COLUMNS: ['survey_id', 'bathrooms', 'city', 'country', 'location']\n",
      "\n",
      "reykjavik's dataset length: 44639\n"
     ]
    }
   ],
   "source": [
    "#loading each city's dataset to a dictionary and stripping their column titles from unwanted characters\n",
    "#moreover checking which columns are not common for all csv files of a particular city in order to be discarded later\n",
    "#in addition their last_modified column values are stripped\n",
    "datasets = dict()\n",
    "for city in cities:\n",
    "    city_data = pd.DataFrame() #initializing city's dataframe\n",
    "    data_path = glob.glob(os.path.join(os.getcwd(), 'data',city,city,city,'*.csv'))\n",
    "    print('\\n\\n\\n{}\\n'.format(city))\n",
    "    first = True\n",
    "    file_columns = [] #containing the columns that the first file of the particular city contains\n",
    "    for f in data_path:\n",
    "        file_data =pd.read_csv(f,delimiter=\",\",lineterminator=\"\\n\") \n",
    "        old_columns = list(file_data.columns.values)\n",
    "        new_columns = [] #containing the columns after the stripping\n",
    "        for col in old_columns:\n",
    "            new_columns.append(col.strip())\n",
    "        if first:\n",
    "            file_columns = new_columns\n",
    "            first = False\n",
    "        else:\n",
    "            if file_columns == new_columns:\n",
    "                print('no difference in the columns')\n",
    "            else:\n",
    "                print('DIFFERENCE IN THE COLUMNS REPORTED')\n",
    "                print('NOT COMMON COLUMNS: {}'.format(find_not_common_columns(file_columns,new_columns)))      \n",
    "        file_data.columns = new_columns\n",
    "        file_data['last_modified'] = file_data['last_modified'].apply(lambda x: x.strip()) #stripping all the values of 'last_modified' column\n",
    "        city_data = pd.concat([city_data,file_data],ignore_index=True)\n",
    "    print('\\n{}\\'s dataset length: {}'.format(city,len(city_data.index)))\n",
    "    #city_data.fillna(\"(unknown)\",inplace=True)\n",
    "    datasets[city] = city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "edinburgh \n",
      "cols with some missing values--> overall_satisfaction(13.393982944%),city(69.7384655016%),name(69.7411771086%),price(0.00271160703391%),bedrooms(0.139647762246%),accommodates(1.75712135797%),minstay(72.1260354949%),location(69.7384655016%),survey_id(69.7384655016%),room_type(0.00677901758477%)\n",
      "cols with only missing values--> bathrooms,borough,country\n",
      "\n",
      "reykjavik \n",
      "cols with some missing values--> city(78.223078474%),price(0.0201617419745%),bedrooms(9.57010685723%),overall_satisfaction(16.4363000963%),minstay(57.7745917247%),location(78.223078474%),survey_id(78.223078474%),room_type(0.0201617419745%)\n",
      "cols with only missing values--> bathrooms,borough,country\n",
      "\n",
      "helsinki \n",
      "cols with some missing values--> city(64.1407968712%),survey_id(64.1407968712%),name(64.3241261305%),bedrooms(0.0427768271816%),overall_satisfaction(12.1425079443%),minstay(88.1935956979%),location(64.1407968712%),property_type(64.1407968712%)\n",
      "cols with only missing values--> bathrooms,borough,country\n",
      "\n",
      "paris \n",
      "cols with some missing values--> overall_satisfaction(21.7886016711%),city(81.9340546624%),price(0.00142461303948%),bedrooms(0.400316264095%),accommodates(1.10250803126%),minstay(54.2065261523%),location(81.9340546624%),host_id(0.0411713168411%),survey_id(81.9340546624%),room_type(0.0428808524884%)\n",
      "cols with only missing values--> bathrooms,borough,country\n",
      "\n",
      "madrid \n",
      "cols with some missing values--> minstay(55.917185718%),bedrooms(0.118431441355%),overall_satisfaction(16.8041056233%)\n",
      "\n",
      "moscow \n",
      "cols with some missing values--> overall_satisfaction(40.9450353623%),city(62.9225371721%),price(0.0194910062928%),bedrooms(0.192125633458%),accommodates(3.81466837445%),minstay(55.2096675391%),location(62.9225371721%),survey_id(62.9225371721%),room_type(0.0473353009968%)\n",
      "cols with only missing values--> bathrooms,country\n",
      "\n",
      "dublin \n",
      "cols with some missing values--> overall_satisfaction(11.4776796371%),city(65.770669213%),price(0.0127317577783%),bedrooms(0.022280576112%),accommodates(0.617490252248%),minstay(77.9963396196%),location(65.770669213%),survey_id(65.770669213%),room_type(0.0127317577783%)\n",
      "cols with only missing values--> bathrooms,borough,country\n",
      "\n",
      "tallin \n",
      "cols with some missing values--> city(68.7840961627%),bedrooms(0.0184928340268%),overall_satisfaction(12.2422561258%),minstay(87.8687008784%),location(68.7840961627%),survey_id(68.7840961627%)\n",
      "cols with only missing values--> bathrooms,borough,country\n",
      "\n",
      "berlin \n",
      "cols with some missing values--> overall_satisfaction(20.1252536226%),city(84.3397025226%),name(84.3760484399%),bedrooms(2.72194973879%),accommodates(3.80114389788%),minstay(53.9664978512%),location(84.3397025226%),host_id(0.00399405684342%),survey_id(84.3397025226%),room_type(0.00399405684342%)\n",
      "cols with only missing values--> bathrooms,borough,country\n",
      "\n",
      "rome \n",
      "cols with some missing values--> overall_satisfaction(18.6260464313%),city(73.1588849573%),price(0.00690428928972%),bedrooms(0.152239578838%),accommodates(2.11512902391%),minstay(63.3230344351%),location(73.1588849573%),host_id(0.00621386036075%),survey_id(73.1588849573%),room_type(0.0131181496505%)\n",
      "cols with only missing values--> bathrooms,borough,country\n",
      "\n",
      "london \n",
      "cols with some missing values--> overall_satisfaction(24.9769772689%),city(77.8461851335%),price(0.00786609979587%),bedrooms(0.662287231593%),accommodates(2.38170153331%),minstay(55.2453455712%),location(77.8461851335%),host_id(0.0500744401639%),survey_id(77.8461851335%),room_type(0.0579405399598%)\n",
      "cols with only missing values--> bathrooms,country\n",
      "\n",
      "lisbon \n",
      "cols with some missing values--> overall_satisfaction(12.3797997215%),city(68.8449167717%),name(68.8838782413%),price(0.00663173950527%),bedrooms(0.122687180848%),accommodates(2.33188540354%),minstay(71.9775847205%),location(68.8449167717%),survey_id(68.8449167717%),room_type(0.00663173950527%)\n",
      "cols with only missing values--> bathrooms,borough,country\n",
      "\n",
      "amsterdam \n",
      "cols with some missing values--> overall_satisfaction(12.486521341%),city(81.4479466381%),name(81.480999832%),price(0.00650226766585%),bedrooms(0.0709830886855%),accommodates(0.714707587605%),minstay(65.3380366403%),location(81.4479466381%),host_id(0.00108371127764%),survey_id(81.4479466381%),room_type(0.00758597894349%)\n",
      "cols with only missing values--> bathrooms,borough,country\n",
      "\n",
      "copenhagen \n",
      "cols with some missing values--> overall_satisfaction(16.2403297034%),city(64.4203590588%),price(0.283298828989%),bedrooms(0.321114736026%),accommodates(0.802466366276%),minstay(70.6112717041%),location(64.4203590588%),host_id(0.00384568546139%),survey_id(64.4203590588%),room_type(0.28714451445%)\n",
      "cols with only missing values--> bathrooms,borough,country\n",
      "\n",
      "vienna \n",
      "cols with some missing values--> overall_satisfaction(16.0774450719%),city(62.3926351255%),bedrooms(0.0569449058036%),accommodates(2.50557585536%),minstay(76.109239311%),location(62.3926351255%),survey_id(62.3926351255%)\n",
      "cols with only missing values--> bathrooms,country\n"
     ]
    }
   ],
   "source": [
    "#find out which of the columns of each city's dataset have missing values \n",
    "#and delete the columns that have only missing values\n",
    "city_columns = dict() #it is going to conatin the feature list of each city\n",
    "for city in datasets.keys():\n",
    "    city_data = datasets[city]\n",
    "    all_data = len(city_data.index)\n",
    "    columns = city_data.columns.values\n",
    "    city_columns[city] = list(columns)\n",
    "    some_mis = dict()\n",
    "    only_mis = []\n",
    "    for col in columns:\n",
    "        #bools = data[col].to_frame().isin({col: ['(unknown)']}) \n",
    "        bools = city_data[col].to_frame().isnull()\n",
    "        num = len(bools[bools[col] == True].index) #number of mssing values in the particular column\n",
    "        if num > 0: #if there is at least one missing value\n",
    "            if num < len(city_data[col].index): #if not all values are missing\n",
    "                some_mis[col] = num\n",
    "            else:\n",
    "                only_mis.append(col) #if all values of the column are missing\n",
    "    print_str = '\\n{} '.format(city)\n",
    "    if len(some_mis.keys()) > 0:\n",
    "        first = True\n",
    "        for col in some_mis.keys():\n",
    "            if first:\n",
    "                first = False\n",
    "                print_str += '\\ncols with some missing values--> {}({}%)'.format(col,100*float(some_mis[col]/all_data))\n",
    "            else:\n",
    "                print_str += ',{}({}%)'.format(col,100*float(some_mis[col]/all_data))\n",
    "    if len(only_mis) > 0:\n",
    "        print_str += '\\ncols with only missing values--> {}'.format(only_mis[0])\n",
    "        for i in range(1,len(only_mis)):\n",
    "            print_str += ',{}'.format(only_mis[i])\n",
    "        city_data.drop(only_mis, axis=1, inplace=True) #deleting columns with only missing data\n",
    "        datasets[city] = city_data\n",
    "    print(print_str)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([[1, 2, 2, 0], [2, np.nan, 3, 1], [3, 4, 3, 1], [2, 4, 3, 1], [2, 4, 3, 1],\n",
    "                   [3, np.nan, 3, 1], [3, 4, 3, 1]],columns=list('ABCD'))\n",
    "col='B'\n",
    "null_entries = df[['A']][df['B'].isnull()]\n",
    "for index,row in null_entries.iterrows():\n",
    "    same_room_entries = df[[col]][df['A'] == row['A']] #get the rest of the \n",
    "    if len(same_room_entries.index) > 0:\n",
    "        df.set_value(index,col,same_room_entries[col].median())\n",
    "\n",
    "df['B'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "edinburgh\n",
      "\n",
      "price:\t0 values were deleted\n",
      "\n",
      "bedrooms:\t101 values were replaced\t17 were not replaced and were deleted\n",
      "\n",
      "overall_satisfaction:\t9872 values were replaced\t1845 were not replaced and were deleted\n",
      "\n",
      "room_type:\t3 values were replaced\t1 were not replaced and were deleted\n",
      "\n",
      "host_id:\t0 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "2.59416903132% of the edinburgh's dataset was deleted\n",
      "\n",
      "reykjavik\n",
      "\n",
      "price:\t0 values were deleted\n",
      "\n",
      "bedrooms:\t4263 values were replaced\t23 were not replaced and were deleted\n",
      "\n",
      "overall_satisfaction:\t7315 values were replaced\t922 were not replaced and were deleted\n",
      "\n",
      "room_type:\t0 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "host_id:\t0 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "2.18381595513% of the reykjavik's dataset was deleted\n",
      "\n",
      "helsinki\n",
      "\n",
      "price:\t0 values were deleted\n",
      "\n",
      "bedrooms:\t7 values were replaced\t3 were not replaced and were deleted\n",
      "\n",
      "overall_satisfaction:\t1984 values were replaced\t230 were not replaced and were deleted\n",
      "\n",
      "room_type:\t0 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "host_id:\t0 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "1.4444237803% of the helsinki's dataset was deleted\n",
      "\n",
      "paris\n",
      "\n",
      "price:\t0 values were deleted\n",
      "\n",
      "bedrooms:\t2800 values were replaced\t592 were not replaced and were deleted\n",
      "\n",
      "overall_satisfaction:\t152636 values were replaced\t43491 were not replaced and were deleted\n",
      "\n",
      "room_type:\t287 values were replaced\t14 were not replaced and were deleted\n",
      "\n",
      "host_id:\t273 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "6.70484222559% of the paris's dataset was deleted\n",
      "\n",
      "madrid\n",
      "\n",
      "price:\t0 values were deleted\n",
      "\n",
      "bedrooms:\t27 values were replaced\t5 were not replaced and were deleted\n",
      "\n",
      "overall_satisfaction:\t3827 values were replaced\t2577 were not replaced and were deleted\n",
      "\n",
      "room_type:\t0 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "host_id:\t0 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "12.7720617333% of the madrid's dataset was deleted\n",
      "\n",
      "moscow\n",
      "\n",
      "price:\t0 values were deleted\n",
      "\n",
      "bedrooms:\t62 values were replaced\t6 were not replaced and were deleted\n",
      "\n",
      "overall_satisfaction:\t14692 values were replaced\t4033 were not replaced and were deleted\n",
      "\n",
      "room_type:\t10 values were replaced\t5 were not replaced and were deleted\n",
      "\n",
      "host_id:\t0 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "12.7138059819% of the moscow's dataset was deleted\n",
      "\n",
      "dublin\n",
      "\n",
      "price:\t0 values were deleted\n",
      "\n",
      "bedrooms:\t6 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "overall_satisfaction:\t7210 values were replaced\t893 were not replaced and were deleted\n",
      "\n",
      "room_type:\t0 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "host_id:\t0 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "1.4547744373% of the dublin's dataset was deleted\n",
      "\n",
      "tallin\n",
      "\n",
      "price:\t0 values were deleted\n",
      "\n",
      "bedrooms:\t2 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "overall_satisfaction:\t1324 values were replaced\t167 were not replaced and were deleted\n",
      "\n",
      "room_type:\t0 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "host_id:\t0 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "1.56836964688% of the tallin's dataset was deleted\n",
      "\n",
      "berlin\n",
      "\n",
      "price:\t0 values were deleted\n",
      "\n",
      "bedrooms:\t6815 values were replaced\t704 were not replaced and were deleted\n",
      "\n",
      "overall_satisfaction:\t50090 values were replaced\t12253 were not replaced and were deleted\n",
      "\n",
      "room_type:\t9 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "host_id:\t9 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "5.45753216941% of the berlin's dataset was deleted\n",
      "\n",
      "rome\n",
      "\n",
      "price:\t0 values were deleted\n",
      "\n",
      "bedrooms:\t421 values were replaced\t100 were not replaced and were deleted\n",
      "\n",
      "overall_satisfaction:\t53857 values were replaced\t11034 were not replaced and were deleted\n",
      "\n",
      "room_type:\t18 values were replaced\t3 were not replaced and were deleted\n",
      "\n",
      "host_id:\t15 values were replaced\t0 were not replaced and were deleted\n",
      "\n",
      "4.00584522365% of the rome's dataset was deleted\n",
      "\n",
      "london\n",
      "\n",
      "price:\t0 values were deleted\n"
     ]
    }
   ],
   "source": [
    "#dealing with the columns which contain both missing and normal entries\n",
    "columns_to_delete = ['city','location','survey_id','name','minstay','property_type']\n",
    "#city is not needed\n",
    "#location is not needed\n",
    "#survey_id is not needed\n",
    "#name is not needed\n",
    "#minstay have too many null values\n",
    "#overall satisfaction cannot be used both because of the many null entries and because the reviews \n",
    "#are usually not enough to produce reliable assumptions\n",
    "#property_type has too many values\n",
    "\n",
    "delete_null_entries = ['price']\n",
    "#these columns' values are too significant and they might change from time to time even for the same room\n",
    "#as a result it seems wiser to delete the entries where these columns are null rather than replace them\n",
    "#accommodates will be dealt with later\n",
    "\n",
    "replace_nulls_median = ['bedrooms','overall_satisfaction']\n",
    "#given the fact that generally the same room appears multiple times inside each city's dataset these columns are going \n",
    "#to be replaced by the median of the column values inside the same room through time\n",
    "\n",
    "replace_nulls_value = ['room_type','host_id']\n",
    "#if the same room_id exists in the dataset with a non-null value for these fields then the null value is replaced \n",
    "#by that. If not the entries containing null values are removed\n",
    "\n",
    "for city in datasets.keys():\n",
    "    print('\\n{}'.format(city))\n",
    "    city_data = datasets[city]\n",
    "    for col in columns_to_delete:\n",
    "        if col in city_data.columns:\n",
    "            city_data.drop([col], axis=1, inplace=True) #columns_to_delete are deleted\n",
    "    old_num = len(city_data.index)\n",
    "    \n",
    "    for col in delete_null_entries:\n",
    "        if col in city_data.columns:\n",
    "            city_data.dropna(axis=0,subset = [col],inplace=True) #the null entries of delete_null_entries are deleted\n",
    "        print('\\n{}:\\t{} values were deleted'.format(\n",
    "                col,len(city_data[city_data[col].isnull()].index)))\n",
    "            \n",
    "    for col in replace_nulls_median:\n",
    "        if col in city_data.columns:\n",
    "            null_entries = city_data[['room_id']][city_data[col].isnull()]\n",
    "            count = 0\n",
    "            for index,row in null_entries.iterrows():\n",
    "                same_room_entries = city_data[[col]][city_data['room_id'] == row['room_id']] \n",
    "                #get the rest of the entries of the particular room where the nul appeared\n",
    "                if len(same_room_entries.index) > 0:\n",
    "                    city_data.set_value(index,col,same_room_entries[col].median())\n",
    "                    count += 1\n",
    "                    #replacing the null value with the median of the values for the same room_id\n",
    "            print('\\n{}:\\t{} values were replaced\\t{} were not replaced and were deleted'.format(\n",
    "                col,count,len(city_data[city_data[col].isnull()].index)))\n",
    "            \n",
    "            city_data.dropna(axis=0,subset = [col],inplace=True) #the null entries that were not replaced are deleted\n",
    "    \n",
    "    for col in replace_nulls_value:\n",
    "        if col in city_data.columns:\n",
    "            null_entries = city_data[['room_id']][city_data[col].isnull()]\n",
    "            count = 0\n",
    "            for index,row in null_entries.iterrows():\n",
    "                same_room_entries = city_data[col][city_data['room_id'] == row['room_id']] \n",
    "                #get the rest of the entries of the particular room where the null appeared\n",
    "                if len(same_room_entries.index) > 0:\n",
    "                    city_data.set_value(index,col,same_room_entries.values[0])\n",
    "                    count += 1\n",
    "                    #replacing the null value with the value existing in the \n",
    "                    #same column of the first entry with the same room_id that appears\n",
    "            print('\\n{}:\\t{} values were replaced\\t{} were not replaced and were deleted'.format(\n",
    "                col,count,len(city_data[city_data[col].isnull()].index)))\n",
    "            \n",
    "            city_data.dropna(axis=0,subset = [col],inplace=True) #the null entries that were not replaced are deleted    \n",
    "            \n",
    "            \n",
    "    new_num = len(city_data.index)\n",
    "    print('\\n{}% of the {}\\'s dataset was deleted'.format(100 * float((old_num-new_num)/len(city_data.index)), city))\n",
    "    datasets[city] = city_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datasets['vienna'][datasets['vienna']['overall_satisfaction'] == 5835467]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#find out which columns are existent to all of the cities and which are not\n",
    "all_columns = set() # the union of columns appearing in all of the cities\n",
    "all_common_columns = set() # the intersection of columns appearing in all of the cities\n",
    "first = True\n",
    "for city,columns in city_columns.iteritems():\n",
    "    columns_set = set(columns) #since all the columns of the same city have a unique name\n",
    "    for col in columns:\n",
    "        all_columns.add(col)\n",
    "    if first == False:\n",
    "        mortals = [] #containing the columns that are going to be removed from the intersection\n",
    "        for col in all_common_columns:\n",
    "            if col not in columns_set:\n",
    "                mortals.append(col)\n",
    "        for mor in mortals: \n",
    "            all_common_columns.remove(mor)\n",
    "    else:\n",
    "        for col in columns:\n",
    "            all_common_columns.add(col) #initializing the intersection\n",
    "        first = False\n",
    "\n",
    "all_common_list = list(all_common_columns) #set to list\n",
    "if len(all_common_list) > 0:\n",
    "    print_str = 'Features appearing to every city of the dataset:\\n{}'.format(all_common_list[0])\n",
    "    for i in range(1,len(all_common_list)): #the rest\n",
    "        print_str += ',{}'.format(all_common_list[i])\n",
    "    print(print_str)\n",
    "else:\n",
    "    print(\"There are no common features between the different cities\")\n",
    "\n",
    "\n",
    "print_str = \"\\nFeatures appearing only to some cities of the dataset:\\n\"\n",
    "first = True\n",
    "for col in all_columns:\n",
    "    if(col not in all_common_columns):\n",
    "        if first == True:\n",
    "            print_str += col\n",
    "            first = False\n",
    "        else:\n",
    "            print_str += \",\" + col\n",
    "\n",
    "print(print_str)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[np.nan, 2, np.nan, 0], [3, 4, np.nan, 1]],columns=list('ABCD'))\n",
    "df\n",
    "#df.dropna(axis=0,subset=['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checking if some bad values exist in accommodates column for every city\n",
    "bad_accommodates_value_found = False\n",
    "error_cities = [] #containing the cities containing bad values in that column\n",
    "for city,data in datasets.iteritems():\n",
    "    print('\\n{}:\\n'.format(city))\n",
    "    error_found = False\n",
    "    bools = data['accommodates'].to_frame().isnull()\n",
    "    numeric_error_entries = len(data[data['accommodates'] < 1].index) #number of entries with values less than 1\n",
    "    missing_entries = len(bools[bools['accommodates'] == True].index) #number of entries with missing values\n",
    "    if numeric_error_entries > 0:\n",
    "        error_found = True\n",
    "        print('Numeric error for {} entries ({}% of the city\\'s dataset)'.format(numeric_error_entries,\n",
    "                                                                                100*float(numeric_error_entries/\n",
    "                                                                                         len(data.index))))\n",
    "    if missing_entries > 0:\n",
    "        error_found = True\n",
    "        print('Missing value for {} entries ({}% of the city\\'s dataset)'.format(missing_entries,\n",
    "                                                                                100*float(missing_entries/\n",
    "                                                                                         len(data.index))))\n",
    "    if error_found:\n",
    "        error_cities.append(city)\n",
    "        bad_accommodates_value_found = True    \n",
    "\n",
    "    \n",
    "#deleting the bad value entries:\n",
    "print('\\n\\n')\n",
    "for city in error_cities:\n",
    "    city_data = datasets[city]\n",
    "    old_num = len(city_data.index)\n",
    "    city_data = city_data[city_data['accommodates'] >= 1]\n",
    "    city_data = city_data.dropna(axis=0,subset = ['accommodates'])\n",
    "    new_num = len(city_data.index)\n",
    "    datasets[city] = city_data\n",
    "    print('{} entries were deleted from {}'.format(old_num - new_num,city))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create new column: price per number of people that can be accommodated\n",
    "for city in datasets.keys():\n",
    "    city_data = datasets[city]\n",
    "    city_data['price_per_accommodated'] = city_data['price'] / city_data['accommodates']\n",
    "    datasets[city] = city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create new column: price per number of people that can be accommodated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input: p--> price, quartiles --> quartiles of price_per|_accommodated column,\n",
    "#k --> tukey test's hyperparameter\n",
    "#output: whether p is a low outlier, a high outlier or a normal instance\n",
    "def tukey_test(p,quartiles,k):\n",
    "    iqr = quartiles[2] - quartiles[0]\n",
    "    lower_bound = quartiles[0] - k * iqr\n",
    "    if(lower_bound < 1):\n",
    "        lower_bound = 1 #we will not use the lower bound of tukey test but a fixed positive lower bound\n",
    "        \n",
    "    upper_bound  = quartiles[2] + k * iqr\n",
    "    low_outliers = 0\n",
    "    high_outliers = 0\n",
    "    if p < lower_bound:\n",
    "        return \"low_outlier\"\n",
    "    elif p > upper_bound:\n",
    "        return \"high_outlier\"\n",
    "    return \"no\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checking if 'last_modified' alone can be used as the primary key\n",
    "for city,data in datasets.iteritems():\n",
    "    print(city)\n",
    "    gb = data.groupby(['last_modified'])\n",
    "    print(gb['last_modified'].count().sort_values(ascending=False)[0])\n",
    "#for some cities it cannot be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checking if 'last_modified' along with 'room_id' can be used as the primary key\n",
    "for city,data in datasets.iteritems():\n",
    "    print(city)\n",
    "    print(data.groupby(['last_modified','room_id']).size().sort_values(ascending=False)[0])\n",
    "#they can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#given the fact that some prices might be per month outlier detection method tukey test will be used\n",
    "#so that these entries will be captures as outliers along with the rest of the outliers the dataset might have\n",
    "#price_per_accommodated will be used instead of price column\n",
    "\n",
    "k = 1.5 #tukey test's hyperparameter\n",
    "all_low_outliers = 0\n",
    "all_high_outliers = 0\n",
    "all_data = 0\n",
    "print('OUTLIER DETECTION AND DELETION')\n",
    "\n",
    "for city in datasets.keys():\n",
    "    mortal_indexes = []\n",
    "    city_data = datasets[city]\n",
    "    quartiles = []\n",
    "    quartiles.append(city_data['price_per_accommodated'].to_frame().quantile(q=0.25,axis=0)[0])\n",
    "    quartiles.append(city_data['price_per_accommodated'].to_frame().mean(axis=0)[0])\n",
    "    quartiles.append(city_data['price_per_accommodated'].to_frame().quantile(q=0.75,axis=0)[0])\n",
    "    city_low_outliers = 0\n",
    "    city_high_outliers = 0\n",
    "    city_data.reset_index(inplace=True)\n",
    "    for index,row in city_data.iterrows():\n",
    "        res = tukey_test(row['price_per_accommodated'],quartiles,k)\n",
    "        if res == 'low_outlier':\n",
    "            city_low_outliers += 1\n",
    "            mortal_indexes.append(index)\n",
    "            #mortal_room_id.append(row['room_id'])\n",
    "            #mortal_last_modified.append(row['last_modified'])\n",
    "        elif res == 'high_outlier':\n",
    "            city_high_outliers += 1\n",
    "            mortal_indexes.append(index)\n",
    "            #mortal_room_id.append(row['room_id'])\n",
    "            #mortal_last_modified.append(row['last_modified'])\n",
    "    all_data += len(city_data.index)\n",
    "    all_low_outliers += city_low_outliers\n",
    "    all_high_outliers += city_high_outliers\n",
    "    city_data = city_data.drop(city_data.index[mortal_indexes]) #deleting the outliers by their index\n",
    "    \n",
    "    datasets[city] = city_data\n",
    "    print('{0}\\t(LOW)\\t{1}\\t(HIGH)\\t{2}\\t(TOTAL)\\t{3}\\t{4}% of city\\'s dataset'.\n",
    "          format(city,city_low_outliers,city_high_outliers,city_low_outliers + city_high_outliers\n",
    "                 ,100 * float((city_low_outliers + city_high_outliers)/len(city_data.index))))\n",
    "\n",
    "print('\\n\\n\\nALTOGETHER\\t(LOW)\\t{0}\\t(HIGH)\\t{1}\\t(TOTAL)\\t{2}\\t{3}% of all cities\\' datasets'.\n",
    "          format(all_low_outliers,all_high_outliers,all_low_outliers + all_high_outliers\n",
    "                 ,100 * float((all_low_outliers + all_high_outliers)/all_data)))\n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checking if 'last_modified' alone can be used as the primary key\n",
    "for city,data in datasets.iteritems():\n",
    "    print(city)\n",
    "    gb = data.groupby(['room_type'])\n",
    "    print(gb['room_type'].count().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating bar chart showing the average prices and average prices per accommodated for all room types, per city\n",
    "plt.clf()\n",
    "N = len(datasets.keys())\n",
    "price_means = []\n",
    "price_stds = []\n",
    "price_per_accommodated_means = []\n",
    "price_per_accommodated_stds = []\n",
    "cities = []\n",
    "\n",
    "for city in datasets.keys():\n",
    "    city_data = datasets[city]\n",
    "    price_means.append(city_data['price'].mean())\n",
    "    price_stds.append(city_data['price'].std())\n",
    "    price_per_accommodated_means.append(city_data['price_per_accommodated'].mean())\n",
    "    price_per_accommodated_stds.append(city_data['price_per_accommodated'].std())\n",
    "    cities.append(city)\n",
    "\n",
    "\n",
    "sort_indices = np.argsort(np.asarray(price_per_accommodated_means)) #sort cities on price per accommodated\n",
    "price_means = list(np.asarray(price_means)[sort_indices[::-1]])\n",
    "price_stds = list(np.asarray(price_stds)[sort_indices[::-1]])\n",
    "price_per_accommodated_means = list(np.asarray(price_per_accommodated_means)[sort_indices[::-1]])\n",
    "price_per_accommodated_stds = list(np.asarray(price_per_accommodated_stds)[sort_indices[::-1]])\n",
    "cities = list(np.asarray(cities)[sort_indices[::-1]])\n",
    "\n",
    "\n",
    "pos = np.arange(N)\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,3))\n",
    "\n",
    "prices_bar = ax.bar(pos, tuple(price_means), width, color='b', yerr=tuple(price_stds))\n",
    "prices_pa_bar = ax.bar(pos+width, tuple(price_per_accommodated_means), width, \n",
    "                       color='g', yerr=tuple(price_per_accommodated_stds))\n",
    "\n",
    "ax.legend((prices_bar[0], prices_pa_bar[0]), ('Price', 'Price per Accommodated'))\n",
    "\n",
    "ax.set_ylabel('Dollars($)')\n",
    "ax.set_title('Average price and price per person, per city')\n",
    "ax.set_xticks(pos + width / 2)\n",
    "ax.set_xticklabels(tuple(cities))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating bar chart showing the average prices and average prices per accommodated for all room types, per city\n",
    "plt.clf()\n",
    "N = len(datasets.keys())\n",
    "price_means = []\n",
    "price_stds = []\n",
    "price_per_accommodated_means = []\n",
    "price_per_accommodated_stds = []\n",
    "cities = []\n",
    "\n",
    "for city in datasets.keys():\n",
    "    city_data = datasets[city]\n",
    "    city_data = city_data[city_data['room_type'] == 'Entire home/apt']\n",
    "    price_means.append(city_data['price'].mean())\n",
    "    price_stds.append(city_data['price'].std())\n",
    "    price_per_accommodated_means.append(city_data['price_per_accommodated'].mean())\n",
    "    price_per_accommodated_stds.append(city_data['price_per_accommodated'].std())\n",
    "    cities.append(city)\n",
    "\n",
    "\n",
    "sort_indices = np.argsort(np.asarray(price_per_accommodated_means)) #sort cities on price per accommodated\n",
    "price_means = list(np.asarray(price_means)[sort_indices[::-1]])\n",
    "price_stds = list(np.asarray(price_stds)[sort_indices[::-1]])\n",
    "price_per_accommodated_means = list(np.asarray(price_per_accommodated_means)[sort_indices[::-1]])\n",
    "price_per_accommodated_stds = list(np.asarray(price_per_accommodated_stds)[sort_indices[::-1]])\n",
    "cities = list(np.asarray(cities)[sort_indices[::-1]])\n",
    "\n",
    "\n",
    "pos = np.arange(N)\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,3))\n",
    "\n",
    "prices_bar = ax.bar(pos, tuple(price_means), width, color='b', yerr=tuple(price_stds))\n",
    "prices_pa_bar = ax.bar(pos+width, tuple(price_per_accommodated_means), width, \n",
    "                       color='g', yerr=tuple(price_per_accommodated_stds))\n",
    "\n",
    "ax.legend((prices_bar[0], prices_pa_bar[0]), ('Price', 'Price per Accommodated'))\n",
    "\n",
    "ax.set_ylabel('Dollars($)')\n",
    "ax.set_title('Average Price and price per person for \\'entire home/apt\\' room type, per city')\n",
    "ax.set_xticks(pos + width / 2)\n",
    "ax.set_xticklabels(tuple(cities))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating bar chart showing the average prices and average prices per accommodated for all room types, per city\n",
    "plt.clf()\n",
    "N = len(datasets.keys())\n",
    "price_means = []\n",
    "price_stds = []\n",
    "price_per_accommodated_means = []\n",
    "price_per_accommodated_stds = []\n",
    "cities = []\n",
    "\n",
    "for city in datasets.keys():\n",
    "    city_data = datasets[city]\n",
    "    city_data = city_data[city_data['room_type'] == 'Private room']\n",
    "    price_means.append(city_data['price'].mean())\n",
    "    price_stds.append(city_data['price'].std())\n",
    "    price_per_accommodated_means.append(city_data['price_per_accommodated'].mean())\n",
    "    price_per_accommodated_stds.append(city_data['price_per_accommodated'].std())\n",
    "    cities.append(city)\n",
    "\n",
    "\n",
    "sort_indices = np.argsort(np.asarray(price_per_accommodated_means)) #sort cities on price per accommodated\n",
    "price_means = list(np.asarray(price_means)[sort_indices[::-1]])\n",
    "price_stds = list(np.asarray(price_stds)[sort_indices[::-1]])\n",
    "price_per_accommodated_means = list(np.asarray(price_per_accommodated_means)[sort_indices[::-1]])\n",
    "price_per_accommodated_stds = list(np.asarray(price_per_accommodated_stds)[sort_indices[::-1]])\n",
    "cities = list(np.asarray(cities)[sort_indices[::-1]])\n",
    "\n",
    "\n",
    "pos = np.arange(N)\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,3))\n",
    "\n",
    "prices_bar = ax.bar(pos, tuple(price_means), width, color='b', yerr=tuple(price_stds))\n",
    "prices_pa_bar = ax.bar(pos+width, tuple(price_per_accommodated_means), width, \n",
    "                       color='g', yerr=tuple(price_per_accommodated_stds))\n",
    "\n",
    "ax.legend((prices_bar[0], prices_pa_bar[0]), ('Price', 'Price per Accommodated'))\n",
    "\n",
    "ax.set_ylabel('Dollars($)')\n",
    "ax.set_title('Average Price and price per person for \\'private room\\' room type, per city')\n",
    "ax.set_xticks(pos + width / 2)\n",
    "ax.set_xticklabels(tuple(cities))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating bar chart showing the average prices and average prices per accommodated for all room types, per city\n",
    "plt.clf()\n",
    "N = len(datasets.keys())\n",
    "price_means = []\n",
    "price_stds = []\n",
    "price_per_accommodated_means = []\n",
    "price_per_accommodated_stds = []\n",
    "cities = []\n",
    "\n",
    "for city in datasets.keys():\n",
    "    city_data = datasets[city]\n",
    "    city_data = city_data[city_data['room_type'] == 'Shared room']\n",
    "    price_means.append(city_data['price'].mean())\n",
    "    price_stds.append(city_data['price'].std())\n",
    "    price_per_accommodated_means.append(city_data['price_per_accommodated'].mean())\n",
    "    price_per_accommodated_stds.append(city_data['price_per_accommodated'].std())\n",
    "    cities.append(city)\n",
    "\n",
    "\n",
    "sort_indices = np.argsort(np.asarray(price_per_accommodated_means)) #sort cities on price per accommodated\n",
    "price_means = list(np.asarray(price_means)[sort_indices[::-1]])\n",
    "price_stds = list(np.asarray(price_stds)[sort_indices[::-1]])\n",
    "price_per_accommodated_means = list(np.asarray(price_per_accommodated_means)[sort_indices[::-1]])\n",
    "price_per_accommodated_stds = list(np.asarray(price_per_accommodated_stds)[sort_indices[::-1]])\n",
    "cities = list(np.asarray(cities)[sort_indices[::-1]])\n",
    "\n",
    "\n",
    "pos = np.arange(N)\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,3))\n",
    "\n",
    "prices_bar = ax.bar(pos, tuple(price_means), width, color='b', yerr=tuple(price_stds))\n",
    "prices_pa_bar = ax.bar(pos+width, tuple(price_per_accommodated_means), width, \n",
    "                       color='g', yerr=tuple(price_per_accommodated_stds))\n",
    "\n",
    "ax.legend((prices_bar[0], prices_pa_bar[0]), ('Price', 'Price per Accommodated'))\n",
    "\n",
    "ax.set_ylabel('Dollars($)')\n",
    "ax.set_title('Average Price and price per person for \\'shared room\\' room type, per city')\n",
    "ax.set_xticks(pos + width / 2)\n",
    "ax.set_xticklabels(tuple(cities))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets['madrid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert last modified column to datetime\n",
    "for city in datasets.keys():\n",
    "    city_data = datasets[city]\n",
    "    city_data['last_modified'] = pd.to_datetime(city_data['last_modified'])\n",
    "    city_data = city_data.sort_values(by='last_modified',ascending=True)\n",
    "    datasets[city] = city_data.reset_index()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets['madrid'].head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#obtain the room_ids with most frequencies per city and per room_type\n",
    "N = 7 #number of most frequent room_ids we are going to use\n",
    "room_types = data.groupby(['room_type']).groups.keys() #get all room_types\n",
    "city_room_data = [] #containing the lists of most frequent rooms_ids per room_type per city\n",
    "for city in cities: #used insead of simple dict iteration so that it is guaranteed that we always have the same order\n",
    "    data = datasets[city]\n",
    "    city_room = [] #containing the lists of most frequent room ids per room type\n",
    "    for rt in room_types:\n",
    "        gb = data[data['room_type'] == rt].groupby(['room_id']) #group by rooms_ids of a particular room_type\n",
    "        city_room.append(list(gb['room_id'].count().sort_values(ascending=False).to_frame().index[:N].values.tolist()))\n",
    "        #getting the first three room_ids with the biggest frequency per room_type\n",
    "    city_room_data.append(city_room)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotting the price changes per time for the most frequent room_ids per city through time\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(len(datasets.keys()),len(room_types), figsize=(20,60))\n",
    "i = 0\n",
    "for city in cities:\n",
    "    data = datasets[city]\n",
    "    data = data.set_index('last_modified')\n",
    "    for j in range(len(room_types)):\n",
    "        for rid in city_room_data[i][j]: #for each room id of ith city and jth room type\n",
    "            data[data['room_id'] == rid]['price'].plot(ax=ax[i][j])\n",
    "            ax[i][j].xaxis.label.set_visible(False)\n",
    "    \n",
    "    i += 1\n",
    "[ax[i][0].set_ylabel(cities[i]) for i in range(len(cities))]\n",
    "[ax[0][j].set_title(room_types[j]) for j in range(len(room_types))]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotting the price_per_accommodated changes per time for the most frequent room_ids per room_type per city through time\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(len(datasets.keys()),len(room_types), figsize=(20,60))\n",
    "i = 0\n",
    "for city in cities:\n",
    "    data = datasets[city]\n",
    "    data = data.set_index('last_modified')\n",
    "    for j in range(len(room_types)):\n",
    "        for rid in city_room_data[i][j]: #for each room id of ith city and jth room type\n",
    "            data[data['room_id'] == rid]['price_per_accommodated'].plot(ax=ax[i][j])\n",
    "            ax[i][j].xaxis.label.set_visible(False)\n",
    "    \n",
    "    i += 1\n",
    "[ax[i][0].set_ylabel(cities[i]) for i in range(len(cities))]\n",
    "[ax[0][j].set_title(room_types[j]) for j in range(len(room_types))]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "data = datasets['madrid']\n",
    "data = data.set_index('last_modified')\n",
    "\n",
    "data1 = data[data['room_type'] == 'Entire home/apt'][['price','price_per_accommodated']].resample('A').mean()\n",
    "data2 = data[data['room_type'] == 'Entire home/apt'][['price','price_per_accommodated']].resample('A').std()\n",
    "fig,ax = subplots()\n",
    "\n",
    "\n",
    "data1.plot.bar(yerr=data2,ax=ax)\n",
    "#data[data['reviews'] > 2].resample('A').mean().head(5)\n",
    "#data[data['room_type'] == 'Entire home/apt'].resample('A').std()['price'].plot(kind='bar',err)\n",
    "#xticks = []\n",
    "#for ind in list(data1.index):\n",
    "#    xticks.append(str(ind).split('-')[0])\n",
    "#ax.set_xticklabels(tuple(xticks))\n",
    "ax.set_xticklabels(tuple([str(ind).split('-')[0] for ind in list(data1.index)]))\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotting the price and price_per_accommodated changes per time for the most frequent room_ids per room_type per city through time\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(len(datasets.keys()),len(room_types), figsize=(20,60))\n",
    "i = 0\n",
    "for city in cities:\n",
    "    data = datasets[city]\n",
    "    data = data.set_index('last_modified')\n",
    "    for j in range(len(room_types)):\n",
    "        means = data[data['room_type'] == room_types[j]][['price','price_per_accommodated']].resample('A').mean()\n",
    "        stds = data[data['room_type'] == room_types[j]][['price','price_per_accommodated']].resample('A').std()\n",
    "        means.dropna().plot.bar(yerr=stds,ax=ax[i][j])\n",
    "        ax[i][j].set_xticklabels(tuple([str(label).split('-')[0] for label in list(means.index)]))\n",
    "        ax[i][j].xaxis.label.set_visible(False)\n",
    "        \n",
    "    \n",
    "    i += 1\n",
    "[ax[i][0].set_ylabel(cities[i]) for i in range(len(cities))]\n",
    "[ax[0][j].set_title(room_types[j]) for j in range(len(room_types))]\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#which percentage of the rooms of each city has above 2 reviews\n",
    "for city,data in datasets.iteritems():\n",
    "    print('{}% of {}\\'s rooms have above 2 reviews'.format\n",
    "          (100 * float(len(data[data['reviews'] > 2].index)/len(data.index)),city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = datasets['madrid']\n",
    "#print(data.groupby(['neighborhood']).groups)\n",
    "#print(len(data.groupby(['borough']).groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checking the diversity in the nominal columns\n",
    "#wherever there is only one value the column is deleted \n",
    "nominal_position_columns = ['neighborhood','borough']\n",
    "\n",
    "for city in datasets.keys():\n",
    "    data = datasets[city]\n",
    "    for col in nominal_position_columns:\n",
    "        if col in set(data.columns):\n",
    "            num = len(data.groupby([col]).groups)\n",
    "            print('\\n{}: There are {} different {}s'.format(city,num,col))\n",
    "            if num < 2:\n",
    "                data.drop([col], axis=1, inplace=True)\n",
    "                print('Column {} was deleted from {}\\'s dataset'.format(col,city))\n",
    "    datasets[city] = data\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#finding out the percentage of the rooms that are presented to have 0 bedrooms\n",
    "for city,data in datasets.iteritems():\n",
    "    print('{}% of {}\\'s rooms are presented to have zero bedrooms'.format(\n",
    "        100 * len(data[data['bedrooms'] == 0]) / len(data.index),city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mapping latitude and longitude to three dimensions(sphere) so that they can be standardized\n",
    "#also transforming datetime to timestamp\n",
    "new_datasets = dict() #containing he new data which only contain the \n",
    "for city in datasets.keys():\n",
    "    data = datasets[city]\n",
    "    new_data = data\n",
    "    new_data['x'] = np.cos(new_data['latitude']) * np.cos(new_data['longitude'])\n",
    "    new_data['y'] = np.cos(new_data['latitude']) * np.sin(new_data['longitude'])\n",
    "    new_data['z'] = np.sin(new_data['latitude'])\n",
    "    new_data = new_data.drop(['latitude','longitude'],axis=1) #deleting former coordinates\n",
    "    for col in nominal_position_columns:\n",
    "        if col in set(new_data.columns):\n",
    "            new_data = new_data.drop([col],axis=1) #deleting nominal position features\n",
    "    print('{}\\'s coordinates system was transformed'.format(city))\n",
    "    new_datasets[city] = new_data #inserted into new data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#converting datetimes to timedeltas so that they can be standardized\n",
    "for city in new_datasets.keys():\n",
    "    data = new_datasets[city]\n",
    "    data['last_modified'] = pd.to_timedelta(\n",
    "    data['last_modified']).apply(lambda x: str(x).split()[0])\n",
    "    print('{}\\'s datetimes were converted to timedeltas'.format(city))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dropping room_id,host_id and other unused for the model to construct columns\n",
    "#moreover room_type columns gets one-hot encoded\n",
    "columns_to_drop = ['level_0','index','room_id','host_id','price_per_accommodated']\n",
    "columns_to_onehot = ['room_type']\n",
    "\n",
    "for city in new_datasets.keys():\n",
    "    data = new_datasets[city]\n",
    "    for col in columns_to_drop: #drop columns_to_drop\n",
    "        if col in set(data.columns):\n",
    "            data = data.drop([col],axis=1)\n",
    "    for col in columns_to_onehot: #replace columns_to_onehot with their binary encoding\n",
    "        if col in set(data.columns):\n",
    "            one_hot_col = pd.get_dummies(data[col]) \n",
    "            data = data.join(one_hot_col)\n",
    "            data = data.drop([col],axis=1) \n",
    "    print('Column deletes and replacements were completed for {}\\'s dataset'.format(city))\n",
    "    new_datasets[city] = data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#***********************deleting rooms with bedrooms = 0\n",
    "#no_bedrooms_datasets = dict()\n",
    "#for city in new_datasets:\n",
    "#    data = new_datasets[city]\n",
    "#    no_bedrooms_datasets[city] = data.drop(data[data['bedrooms'] == 0].index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#transfering price to separate unit-column dataframe\n",
    "prices = dict() #containing the price column (that is going to be predicted) for each city\n",
    "for city in new_datasets.keys():\n",
    "    data = new_datasets[city]\n",
    "    price_data = pd.DataFrame()\n",
    "    price_data['price'] = data['price']\n",
    "    data = data.drop(['price'],axis=1)\n",
    "    prices[city] = price_data\n",
    "    new_datasets[city] = data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prices['madrid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_datasets['reykjavik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#splitting each cities dataset to training and test set\n",
    "training_datasets = dict() #containing the training sets of each city\n",
    "training_prices = dict() #containing the targets of the training set of each city\n",
    "test_datasets = dict() #containing the test sets of each city\n",
    "test_prices = dict() #containing the targets of the test set of each city\n",
    "\n",
    "for city in new_datasets.keys():\n",
    "    X = new_datasets[city]\n",
    "    y = prices[city]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=random_state)\n",
    "    training_datasets[city] = X_train\n",
    "    training_prices[city] = y_train\n",
    "    test_datasets[city] = X_test\n",
    "    test_prices[city] = y_test\n",
    "    print('{}:\\tInitial size:\\t{}\\tTraining set size:\\t{}\\tTest set size:\\t{}'.format(city,len(X.index),len(X_train.index),\n",
    "                                                                                len(X_test.index)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_datasets['helsinki']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initializing 3-fold cross-validation\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generating 3 training-validation set pairs from each city's training set\n",
    "X_trains = dict() #containing the list of the training sets per city\n",
    "X_vals = dict() #containing the list of the validation sets per city\n",
    "y_trains = dict() #containing the list of the training sets' targets per city\n",
    "y_vals = dict() #containing the list of the validation sets' targets per city\n",
    "\n",
    "for city in training_datasets.keys():\n",
    "    X = training_datasets[city]\n",
    "    y = training_prices[city]\n",
    "    X_trs = [] #the list of the training sets per city\n",
    "    X_vs = [] #the list of the validation sets per city\n",
    "    y_trs = [] #the list of the training sets' targets per city\n",
    "    y_vs = [] #the list of the validation sets' targets per city\n",
    "    for train_index, val_index in cv.split(X): #splitting to training and validation set\n",
    "        X_trs.append(X.iloc[train_index])\n",
    "        y_trs.append(y.iloc[train_index])\n",
    "        X_vs.append(X.iloc[val_index])\n",
    "        y_vs.append(y.iloc[val_index])\n",
    "    X_trains[city] = X_trs\n",
    "    X_vals[city] = X_vs\n",
    "    y_trains[city] = y_trs\n",
    "    y_vals[city] = y_vs\n",
    "    print('{}:\\tInitial size of training data:\\t{}\\tTraining sets\\' size:\\t{}\\tValidation sets\\' size:\\t{}'.format(city,len(X.index),len(X_trs[0].index),\n",
    "                                                                                len(X_vs[0].index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_trains['paris'][0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#standardizing the non-binary features as well as the targets(price)\n",
    "\n",
    "for city in X_trains.keys():\n",
    "    X_trs = X_trains[city]\n",
    "    X_vs = X_vals[city]\n",
    "    y_trs = y_trains[city]\n",
    "    y_vs = y_vals[city]\n",
    "    X_columns = list(X_trs[0]) #holdinh the column names of the dataset\n",
    "    y_column = list(y_trs[0]) #holding the column name of the targets\n",
    "    X_trs_sc = [] #they are going to host the standardized versions\n",
    "    X_vs_sc = []\n",
    "    y_trs_sc = []\n",
    "    y_vs_sc = []\n",
    "    stand_columns = [col for col in X_columns if col not in set(room_types)] \n",
    "    #holding the names of the columns to be standardized\n",
    "        \n",
    "    \n",
    "    for i in range(len(X_trs)):\n",
    "        X_tr_index = list(X_trs[i].index) #holding the index of the particular cross-validation split\n",
    "        X_v_index = list(X_vs[i].index) #holding the index of the particular cross-validation split\n",
    "        X_tr_stand = X_trs[i][stand_columns] #holding the features to be standardized\n",
    "        X_v_stand = X_vs[i][stand_columns] #holding the features to be standardized\n",
    "        X_tr = X_trs[i].drop(stand_columns,axis=1) #holding only the binary columns\n",
    "        X_v = X_vs[i].drop(stand_columns,axis=1) #holding only the binary columns\n",
    "        non_stand_columns = list(X_tr.columns) #holding the names of the binary columns\n",
    "        X_sc = StandardScaler().fit(X_tr_stand) \n",
    "        X_tr_stand = X_sc.transform(X_tr_stand) #standardization of the training set\n",
    "        X_v_stand = X_sc.transform(X_v_stand) #standardization of the validation set using the model trained on \n",
    "                                                    #the training set\n",
    "        X_tr = np.c_[X_tr_stand,X_tr] #uniting standardized features with binary features\n",
    "        X_v = np.c_[X_v_stand,X_v] #uniting standardized features with binary features\n",
    "            \n",
    "        X_trs_sc.append(pd.DataFrame(X_tr,columns=stand_columns+non_stand_columns,index=X_tr_index))\n",
    "        X_vs_sc.append(pd.DataFrame(X_v,columns=stand_columns+non_stand_columns,index=X_v_index))\n",
    "        \n",
    "        y_tr = y_trs[i].values\n",
    "        y_v = y_vs[i].values\n",
    "        y_sc = StandardScaler().fit(y_tr) \n",
    "        y_tr = y_sc.transform(y_tr) #standardization of the training targets\n",
    "        y_v = y_sc.transform(y_v) #standardization of the validation targets using training targets' model\n",
    "        y_trs_sc.append(pd.DataFrame(y_tr,columns=y_column,index=X_tr_index))\n",
    "        y_vs_sc.append(pd.DataFrame(y_v,columns=y_column,index=X_v_index))\n",
    "    \n",
    "    \n",
    "    X_trains[city] = X_trs_sc\n",
    "    X_vals[city] = X_vs_sc\n",
    "    y_trains[city] = y_trs_sc\n",
    "    y_vals[city] = y_vs_sc\n",
    "    \n",
    "    print('{}\\'s data were standardized'.format(city))\n",
    "y_trains['paris'][0].head(5)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "a = np.array([1,2,3,4])\n",
    "b = np.array([2,3,4,5])\n",
    "l.append(a)\n",
    "l.append(b)\n",
    "l = np.vstack(l)\n",
    "print(np.mean(l,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing stability selection (trying to estimate the value of each ferature as far as the regression is concerned)\n",
    "#input: X_trains_sc --> list of standardized training sets\n",
    "#       y_trains_sc --> list of standardized training targets\n",
    "#output: list containing the mean and standard deviation of the metric \n",
    "def get_features_importance(X_trains_sc,y_trains_sc,selection_threshold=0.25):\n",
    "    rlasso = RandomizedLasso(random_state=random_state,selection_threshold=selection_threshold)\n",
    "    cv_scores = []\n",
    "    for i in range(len(X_trains_sc)):\n",
    "        rlasso.fit(X_trains_sc[i],y_trains_sc[i].values.ravel())\n",
    "        cv_scores.append(rlasso.scores_)\n",
    "    cv_scores = np.asarray(cv_scores)\n",
    "    cv_scores = np.vstack(cv_scores) #list of arrays to array\n",
    "    return [list(np.mean(cv_scores,axis=0)),list(np.std(cv_scores,axis=0))]\n",
    "    #returning a list containing the list containing the means and the stds of the metric per feature\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing Linear Regression\n",
    "#input: X_trains_sc --> list of standardized training sets\n",
    "#       y_trains_sc --> list of standardized training targets\n",
    "#       X_vals_sc --> list of standardized validation sets\n",
    "#       y_vals_sc --> list of standardized validation targets\n",
    "#output: list containing the means and standard deviations of regression evaluation metrics \n",
    "def get_linear_regression(X_trains_sc,y_trains_sc,X_vals_sc,y_vals_sc):\n",
    "    lr = LinearRegression()\n",
    "    cv_mses = []\n",
    "    cv_maes = []\n",
    "    cv_r2s = []\n",
    "    for i in range(len(X_trains_sc)):\n",
    "        lr.fit(X_trains_sc[i],y_trains_sc[i].values.ravel())\n",
    "        y_pred = lr.predict(X_vals_sc[i])\n",
    "        cv_mses.append(math.sqrt(mean_squared_error(y_vals_sc[i],y_pred)))\n",
    "        cv_maes.append(mean_absolute_error(y_vals_sc[i],y_pred))\n",
    "        cv_r2s.append(r2_score(y_vals_sc[i],y_pred))\n",
    "    cv_mses = np.asarray(cv_mses)\n",
    "    cv_maes = np.asarray(cv_maes)\n",
    "    cv_r2s = np.asarray(cv_r2s)\n",
    "    \n",
    "    rmse = [np.mean(cv_mses),np.std(cv_mses)] #list containing the mean and the std of the particular metric\n",
    "    mae = [np.mean(cv_maes),np.std(cv_maes)]\n",
    "    r2 = [np.mean(cv_r2s),np.std(cv_r2s)]\n",
    "    return [rmse,mae,r2] #returning the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing Support Vector Machines Regression\n",
    "#input: X_trains_sc --> list of standardized training sets\n",
    "#       y_trains_sc --> list of standardized training targets\n",
    "#       X_vals_sc --> list of standardized validation sets\n",
    "#       y_vals_sc --> list of standardized validation targets\n",
    "#output: list containing the means and standard deviations of regression evaluation metrics \n",
    "def get_svm_regression(X_trains_sc,y_trains_sc,X_vals_sc,y_vals_sc):\n",
    "    clf = SVR()\n",
    "    cv_mses = []\n",
    "    cv_maes = []\n",
    "    cv_r2s = []\n",
    "\n",
    "    for i in range(len(X_trains_sc)):\n",
    "        clf.fit(X_trains_sc[i],y_trains_sc[i].values.ravel())\n",
    "        y_pred = clf.predict(X_vals_sc[i])\n",
    "        cv_mses.append(math.sqrt(mean_squared_error(y_vals_sc[i],y_pred)))\n",
    "        cv_maes.append(mean_absolute_error(y_vals_sc[i],y_pred))\n",
    "        cv_r2s.append(r2_score(y_vals_sc[i],y_pred))\n",
    "    cv_mses = np.asarray(cv_mses)\n",
    "    cv_maes = np.asarray(cv_maes)\n",
    "    cv_r2s = np.asarray(cv_r2s)\n",
    "    rmse = [np.mean(cv_mses),np.std(cv_mses)] #list containing the mean and the std of the particular metric\n",
    "    mae = [np.mean(cv_maes),np.std(cv_maes)]\n",
    "    r2 = [np.mean(cv_r2s),np.std(cv_r2s)]\n",
    "    return [rmse,mae,r2] #returning the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing Multi Layer Perceptron Regression\n",
    "#input: X_trains_sc --> list of standardized training sets\n",
    "#       y_trains_sc --> list of standardized training targets\n",
    "#       X_vals_sc --> list of standardized validation sets\n",
    "#       y_vals_sc --> list of standardized validation targets\n",
    "#output: list containing the means and standard deviations of regression evaluation metrics \n",
    "def get_mlp_regression(X_trains_sc,y_trains_sc,X_vals_sc,y_vals_sc):\n",
    "    mlpr = MLPRegressor(random_state=random_state)\n",
    "    cv_mses = []\n",
    "    cv_maes = []\n",
    "    cv_r2s = []\n",
    "    for i in range(len(X_trains_sc)):\n",
    "        mlpr.fit(X_trains_sc[i],y_trains_sc[i].values.ravel())\n",
    "        y_pred = mlpr.predict(X_vals_sc[i])\n",
    "        cv_mses.append(math.sqrt(mean_squared_error(y_vals_sc[i],y_pred)))\n",
    "        cv_maes.append(mean_absolute_error(y_vals_sc[i],y_pred))\n",
    "        cv_r2s.append(r2_score(y_vals_sc[i],y_pred))\n",
    "    cv_mses = np.asarray(cv_mses)\n",
    "    cv_maes = np.asarray(cv_maes)\n",
    "    cv_r2s = np.asarray(cv_r2s)\n",
    "    rmse = [np.mean(cv_mses),np.std(cv_mses)] #list containing the mean and the std of the particular metric\n",
    "    mae = [np.mean(cv_maes),np.std(cv_maes)]\n",
    "    r2 = [np.mean(cv_r2s),np.std(cv_r2s)]\n",
    "    return [rmse,mae,r2] #returning the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing Random Forest Regression\n",
    "#input: X_trains_sc --> list of standardized training sets\n",
    "#       y_trains_sc --> list of standardized training targets\n",
    "#       X_vals_sc --> list of standardized validation sets\n",
    "#       y_vals_sc --> list of standardized validation targets\n",
    "#output: list containing the means and standard deviations of regression evaluation metrics \n",
    "def get_rf_regression(X_trains_sc,y_trains_sc,X_vals_sc,y_vals_sc):\n",
    "    rfr = RandomForestRegressor(random_state=random_state)\n",
    "    cv_mses = []\n",
    "    cv_maes = []\n",
    "    cv_r2s = []\n",
    "    for i in range(len(X_trains_sc)):\n",
    "        rfr.fit(X_trains_sc[i],y_trains_sc[i].values.ravel())\n",
    "        y_pred = rfr.predict(X_vals_sc[i])\n",
    "        cv_mses.append(math.sqrt(mean_squared_error(y_vals_sc[i],y_pred)))\n",
    "        cv_maes.append(mean_absolute_error(y_vals_sc[i],y_pred))\n",
    "        cv_r2s.append(r2_score(y_vals_sc[i],y_pred))\n",
    "    cv_mses = np.asarray(cv_mses)\n",
    "    cv_maes = np.asarray(cv_maes)\n",
    "    cv_r2s = np.asarray(cv_r2s)\n",
    "    rmse = [np.mean(cv_mses),np.std(cv_mses)] #list containing the mean and the std of the particular metric\n",
    "    mae = [np.mean(cv_maes),np.std(cv_maes)]\n",
    "    r2 = [np.mean(cv_r2s),np.std(cv_r2s)]\n",
    "    return [rmse,mae,r2] #returning the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trying to estimate the importance of the features with stability selection\n",
    "selection_threshold=0.3 #the fraction of features selected in each sub-sample of the method\n",
    "for city in X_trains.keys():\n",
    "    print('{}:'.format(city))\n",
    "    X_trs = X_trains[city]\n",
    "    y_trs = y_trains[city]\n",
    "    features = list(X_trs[0]) #holding the names of the features\n",
    "    results = get_features_importance(X_trs,y_trs,selection_threshold)\n",
    "    for i in range(len(results[0])):\n",
    "        print('{0}: {1} +- {2:.4f}'.format(features[i],results[0][i],results[1][i]))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trying and evaluating different classifiers (using all columns)\n",
    "print('\\tRoot Mean Square Error\\tMean Absolute Error\\tR2 Score')\n",
    "for city in X_trains.keys():\n",
    "    print('\\n{}:\\n'.format(city))\n",
    "    X_trs = X_trains[city]\n",
    "    y_trs = y_trains[city]\n",
    "    X_vs = X_vals[city]\n",
    "    y_vs = y_vals[city]\n",
    "    lr_res = get_linear_regression(X_trs,y_trs,X_vs,y_vs)\n",
    "    print('LR',end='')\n",
    "    for res in lr_res:\n",
    "        print('\\t{0:.4f} +- {1:.4f}'.format(res[0],res[1]),end='')\n",
    "    print()\n",
    "    #svm_res = get_svm_regression(X_trs,y_trs,X_vs,y_vs)\n",
    "    #print('SVM',end='')\n",
    "    #for res in svm_res:\n",
    "        #print('\\t{0:.4f} +- {1:.4f}'.format(res[0],res[1]),end='')\n",
    "    #print()\n",
    "    mlp_res = get_mlp_regression(X_trs,y_trs,X_vs,y_vs)\n",
    "    print('MLP',end='')\n",
    "    for res in mlp_res:\n",
    "        print('\\t{0:.4f} +- {1:.4f}'.format(res[0],res[1]),end='')\n",
    "    print()\n",
    "    rf_res = get_rf_regression(X_trs,y_trs,X_vs,y_vs)\n",
    "    print('RF',end='')\n",
    "    for res in rf_res:\n",
    "        print('\\t{0:.4f} +- {1:.4f}'.format(res[0],res[1]),end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
